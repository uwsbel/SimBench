import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

import evaluate
from codebleu import calc_codebleu

from openai import OpenAI
import os
import json
from tqdm import tqdm
import re
import sys
import logging
import csv
import io
import subprocess
rouge = evaluate.load('rouge')

def read_script(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read()

def evaluate_pychrono_code_against_reference_document(code, reference_code, api_documentation_link, model_link):
    prompt = f"""
    You are a PyChrono expert tasked with evaluating a simulation script by comparing it against a reference script generated by experts. Your evaluation should consider both the accuracy of the script compared to the reference and adherence to best practices as outlined in the PyChrono API documentation.

    Here is the PyChrono code you need to evaluate:
    [The Start of Assistant’s Answer]
    {code}
    [The End of Assistant’s Answer]

    Here is the expert-generated reference code:
    [The Start of Reference Answer]
    {reference_code}
    [The End of Reference Answer]

    Use the following evaluation criteria and point deduction guidelines:

    1. **Completeness (40 points total)**
       - Compare the provided code to the reference script. Deduct **15 points** for each missing essential component (e.g., system initialization, body creation, visualization) that is present in the reference script.
       - Deduct **10 points** if a component is present but lacks important details or is incorrectly configured compared to the reference.
       - Deduct **5 points** for minor omissions or slight deviations from the reference script.

    2. **Correctness (30 points total)**
       - Compare the code to the reference script. Deduct **15 points** for each incorrect use of a PyChrono API that could lead to a significant change in simulation behavior.
       - Deduct **10 points** for logical errors in the code, such as incorrect joint initialization or wrong setting of body properties, especially if the reference script does it correctly.
       - Deduct **5 points** for minor inaccuracies or unnecessary API calls that deviate from the reference script.

    3. **Code Quality (10 points total)**
       - Evaluate the readability, structure, and documentation of the code against the reference script. Deduct **5 to 10 points** for poor readability, structure, or lack of meaningful variable names and formatting.
       - Deduct **5 points** for insufficient comments or failure to follow documentation best practices, especially if the reference script provides better documentation.

    4. **Efficiency (10 points total)**
       - Evaluate the efficiency of the code compared to the reference script. Deduct **5 points** for each instance of unnecessary calculations, redundant code, or inefficient use of APIs that is optimized in the reference script.
       - Deduct **3 points** for missing obvious optimization opportunities that the reference script implements.

    5. **Error Handling and Robustness (5 points total)**
       - Assess the error handling and robustness of the code. Deduct **5 points** for lack of basic error handling or failure to account for common issues that the reference script handles.
       - Deduct **3 points** for inadequate handling of edge cases compared to the reference script.

    6. **Use of Visualization Tools (5 points total)**
       - Compare the use of visualization tools in the provided code to the reference script. Deduct **3 to 5 points** for incorrect or inadequate visualization setup as per the reference script.
       - Deduct **2 points** for minor visualization issues, such as suboptimal lighting or incomplete setup of visual elements, compared to the reference.

    Avoid position biases and ensure that the order in which the responses are presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.

    After providing your explanation, output the final score using the following format: "[[x]]" where x is the score assigned to the assistant’s answer.

    Reference the PyChrono API documentation provided here: {api_documentation_link}

    Provide the evaluated score and a brief explanation of the deductions below:
    """
    try:
        global nvidia_api_key
        client = OpenAI(api_key=''
        )
        completion = client.chat.completions.create(
            model=model_link,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            top_p=0.7,
            max_tokens=1000 * 12,
            stream=False
        )
        return completion.choices[0].message.content, prompt
    except Exception as e:
        print('error2:', e)
        return str(e), str(e)

def evaluate_pychrono_code_against_reference(code, reference_code, model_link):
    prompt = f"""
    You are a PyChrono expert tasked with evaluating a simulation script by comparing it against a reference script generated by experts.

    Here is the PyChrono code you need to evaluate:
    [The Start of Assistant’s Answer]
    {code}
    [The End of Assistant’s Answer]

    Here is the expert-generated reference code:
    [The Start of Reference Answer]
    {reference_code}
    [The End of Reference Answer]

    Use the following evaluation criteria and point deduction guidelines:

    1. **Completeness (40 points total)**
       - Compare the provided code to the reference script. Deduct **15 points** for each missing essential component (e.g., system initialization, body creation, visualization) that is present in the reference script.
       - Deduct **10 points** if a component is present but lacks important details or is incorrectly configured compared to the reference.
       - Deduct **5 points** for minor omissions or slight deviations from the reference script.

    2. **Correctness (30 points total)**
       - Compare the code to the reference script. Deduct **15 points** for each incorrect use of a PyChrono API that could lead to a significant change in simulation behavior.
       - Deduct **10 points** for logical errors in the code, such as incorrect joint initialization or wrong setting of body properties, especially if the reference script does it correctly.
       - Deduct **5 points** for minor inaccuracies or unnecessary API calls that deviate from the reference script.

    3. **Code Quality (10 points total)**
       - Evaluate the readability, structure, and documentation of the code against the reference script. Deduct **5 to 10 points** for poor readability, structure, or lack of meaningful variable names and formatting.
       - Deduct **5 points** for insufficient comments or failure to follow documentation best practices, especially if the reference script provides better documentation.

    4. **Efficiency (10 points total)**
       - Evaluate the efficiency of the code compared to the reference script. Deduct **5 points** for each instance of unnecessary calculations, redundant code, or inefficient use of APIs that is optimized in the reference script.
       - Deduct **3 points** for missing obvious optimization opportunities that the reference script implements.

    5. **Error Handling and Robustness (5 points total)**
       - Assess the error handling and robustness of the code. Deduct **5 points** for lack of basic error handling or failure to account for common issues that the reference script handles.
       - Deduct **3 points** for inadequate handling of edge cases compared to the reference script.

    6. **Use of Visualization Tools (5 points total)**
       - Compare the use of visualization tools in the provided code to the reference script. Deduct **3 to 5 points** for incorrect or inadequate visualization setup as per the reference script.
       - Deduct **2 points** for minor visualization issues, such as suboptimal lighting or incomplete setup of visual elements, compared to the reference.

    Avoid position biases and ensure that the order in which the responses are presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.

    After providing your explanation, output the final score using the following format: "[[x]]" where x is the score assigned to the assistant’s answer.

    Provide the evaluated score and a brief explanation of the deductions below:
    """
    try:
        global nvidia_api_key
        client = OpenAI(api_key=''
        )
        completion = client.chat.completions.create(
            model=model_link,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            top_p=0.7,
            max_tokens=1000 * 12,
            stream=False
        )
        return completion.choices[0].message.content, prompt
    except Exception as e:
        print('error2:', e)
        return str(e), str(e)

def evaluate_pychrono_code_against_document(code, api_documentation_link, model_link):
    prompt = f"""
        You are a PyChrono expert tasked with evaluating a simulation script by comparing it against the PyChrono API documentation. While the API documentation provides guidelines, it may not cover all aspects due to length constraints. Therefore, your evaluation should also be based on your knowledge of best practices in Python coding and general simulation principles.

        Here is the PyChrono code you need to evaluate:
        [The Start of Assistant’s Answer]
        {code}
        [The End of Assistant’s Answer]

        Use the following evaluation criteria and point deduction guidelines:

        1. **Completeness (40 points total)**
           - Deduct **15 points** for each missing essential component (e.g., system initialization, body creation, visualization) as outlined in the PyChrono API documentation or generally expected in a simulation setup.
           - Deduct **10 points** if a component is present but lacks important details or is incorrectly configured according to the API documentation or general simulation best practices.
           - Deduct **5 points** for minor omissions or slight deviations from best practices mentioned in the API documentation or common Python coding practices.

        2. **Correctness (30 points total)**
           - Deduct **15 points** for each incorrect use of a PyChrono API that could lead to a significant change in simulation behavior, as indicated by the documentation or your expert knowledge.
           - Deduct **10 points** for logical errors in the code, such as incorrect joint initialization or wrong setting of body properties, based on the API documentation or standard simulation principles.
           - Deduct **5 points** for minor inaccuracies or unnecessary API calls that deviate from the API guidelines or standard coding practices.

        3. **Code Quality (10 points total)**
           - Evaluate the readability, structure, and documentation of the code. Deduct **5 to 10 points** for poor readability, structure, or lack of meaningful variable names and formatting, based on your Python expertise.
           - Deduct **5 points** for insufficient comments or failure to follow documentation best practices, whether outlined in the API documentation or based on general coding standards.

        4. **Efficiency (10 points total)**
           - Deduct **5 points** for each instance of unnecessary calculations, redundant code, or inefficient use of APIs that could be optimized according to the API documentation or your understanding of efficient coding practices.
           - Deduct **3 points** for missing obvious optimization opportunities as suggested by the API documentation or standard programming practices.

        5. **Error Handling and Robustness (5 points total)**
           - Deduct **5 points** for lack of basic error handling or failure to account for common issues, as recommended by the API documentation or best practices in Python coding.
           - Deduct **3 points** for inadequate handling of edge cases, considering both the API documentation and typical robustness requirements in coding.

        6. **Use of Visualization Tools (5 points total)**
           - Deduct **3 to 5 points** for incorrect or inadequate visualization setup according to the API documentation or general expectations for visualizing simulations.
           - Deduct **2 points** for minor visualization issues, such as suboptimal lighting or incomplete setup of visual elements, based on both the API documentation and your understanding of effective simulation visualization.

        Avoid position biases and ensure that the order in which the responses are presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.

        Reference the PyChrono API documentation provided here: {api_documentation_link}

        After providing your explanation, output the final score using the following format: "[[x]]" where x is the score assigned to the assistant’s answer.

        Provide the evaluated score and a brief explanation of the deductions below:
        """
    try:
        global nvidia_api_key
        client = OpenAI(api_key=''
        )
        completion = client.chat.completions.create(
            model=model_link,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            top_p=0.7,
            max_tokens=1000 * 12,
            stream=False
        )
        return completion.choices[0].message.content, prompt
    except Exception as e:
        print('error2:', e)
        return str(e), str(e)

def evaluate_and_save_results(round_name, prediction, reference_code, api_path, output_system_path):
    # Perform first method evaluation (against documentation)
    score_document, prompt_document = evaluate_pychrono_code_against_document(prediction, api_path, evaluated_model)
    print(score_document)

    # Perform second method evaluation (against reference code)
    score_reference, prompt_reference = evaluate_pychrono_code_against_reference(prediction, reference_code, evaluated_model)
    print(score_reference)

    # Perform third method evaluation (against both reference code and documentation)
    score_reference_document, prompt_reference_document = evaluate_pychrono_code_against_reference_document(prediction, reference_code, api_path, evaluated_model)
    print(score_reference_document)

    # Define paths for saving scores
    score_document_path = os.path.join(output_system_path, f"{round_name}_score_document.txt")
    score_reference_path = os.path.join(output_system_path, f"{round_name}_score_reference.txt")
    score_reference_document_path = os.path.join(output_system_path, f"{round_name}_score_reference_document.txt")

    # Save scores to files
    with open(score_document_path, 'w', encoding="utf-8") as file:
        file.write(score_document)

    with open(score_reference_path, 'w', encoding="utf-8") as file:
        file.write(score_reference)

    with open(score_reference_document_path, 'w', encoding="utf-8") as file:
        file.write(score_reference_document)

        # Prepare data to save as JSON
        evaluation_data = {
            "round_name": round_name,
            "prediction": prediction,
            "reference_code": reference_code,
            "api_path": api_path,
            "output_system_path": output_system_path,
            "scores": {
                "score_document": score_document,
                "score_reference": score_reference,
                "score_reference_document": score_reference_document
            },
            "prompts": {
                "prompt_document": prompt_document,
                "prompt_reference": prompt_reference,
                "prompt_reference_document": prompt_reference_document
            }
        }

        # Define path for JSON output
        json_output_path = os.path.join(output_system_path, f"{round_name}_evaluation.json")

        # Save evaluation data to JSON file
        with open(json_output_path, 'w', encoding="utf-8") as json_file:
            json.dump(evaluation_data, json_file, indent=4, ensure_ascii=False)


def merge_csv_files(output_path, combined_csv_filename="combined_evaluation_scores.csv"):
    """
    Merges all small CSV files from different models and systems into a single large CSV file.

    :param output_path: The root directory where all the model-specific directories are stored.
    :param combined_csv_filename: The name of the resulting combined CSV file.
    """
    combined_csv_data = []

    # Iterate through all model directories
    for model_dir in os.listdir(output_path):
        model_path = os.path.join(output_path, model_dir)
        if os.path.isdir(model_path):
            # Iterate through all system directories within each model directory
            for system_dir in os.listdir(model_path):
                system_path = os.path.join(model_path, system_dir)
                if os.path.isdir(system_path):
                    # Path to the small CSV file
                    small_csv_path = os.path.join(system_path, "evaluation_scores.csv")
                    if os.path.exists(small_csv_path):
                        with open(small_csv_path, 'r', encoding="utf-8") as csvfile:
                            reader = csv.reader(csvfile)
                            headers = next(reader)
                            if not combined_csv_data:
                                # Add the header from the first file
                                combined_csv_data.append(headers)
                            # Append the rows from the current small CSV
                            combined_csv_data.extend(list(reader))
                    else:
                        print(f"No CSV file found at {small_csv_path}")

    # Save the combined data into a large CSV file
    combined_csv_path = os.path.join(output_path, combined_csv_filename)
    with open(combined_csv_path, 'w', newline='', encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(combined_csv_data)

    print(f"Combined CSV file saved to {combined_csv_path}")


def extract_scores_from_txt(file_path):
    """
    Extracts the numerical score from a text file.
    Assumes that the score is in the format [[x]] where x is the number.
    """
    with open(file_path, 'r', encoding="utf-8") as file:
        content = file.read()

    # Use regex to find the score in the format [[x]]
    match = re.search(r"\[\[(\d+)\]\]", content)
    if match:
        return int(match.group(1))
    else:
        raise ValueError(f"No valid score found in {file_path}")


def save_scores_to_csv_with_metadata(output_system_path, test_model, system_folder,
                                     csv_filename="evaluation_scores.csv"):
    """
    Extracts scores from text files for different evaluation rounds and saves them into a CSV,
    including metadata like the LLM model, testing model, and system.

    :param output_system_path: The directory containing the score text files.
    :param test_model: The name of the LLM being evaluated.
    :param system_folder: The name of the dynamical system being tested.
    :param csv_filename: The name of the CSV file to save the scores.
    """
    # Prepare a list to hold CSV rows
    csv_data = [["Test Model", "System", "Round", "Score Document", "Score Reference", "Score Reference Document"]]

    rounds = ["first", "second", "third"]
    csv_output_path = os.path.join(output_system_path, csv_filename)

    for round_name in rounds:
        try:
            # Define paths to the score text files for each method
            score_document_path = os.path.join(output_system_path, f"{round_name}_score_document.txt")
            score_reference_path = os.path.join(output_system_path, f"{round_name}_score_reference.txt")
            score_reference_document_path = os.path.join(output_system_path,
                                                         f"{round_name}_score_reference_document.txt")

            # Extract scores from the text files
            score_document = extract_scores_from_txt(score_document_path)
            score_reference = extract_scores_from_txt(score_reference_path)
            score_reference_document = extract_scores_from_txt(score_reference_document_path)

            # Append the scores along with metadata to the CSV data list
            csv_data.append(
                [test_model, system_folder, round_name, score_document, score_reference, score_reference_document])

        except Exception as e:
            print(f"Error processing {round_name} in system {system_folder} for model {test_model}: {e}")

        # Overwrite the CSV file with new data
    with open(csv_output_path, 'w', newline='', encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(csv_data)

    print(f"Scores saved to {csv_output_path}")
# data set path
dataset_path = 'D:\SimBench\demo_data'
Output_path = 'D:\SimBench\output'
Output_conversation_path = 'D:\SimBench\output_conversion'
Output_statistic_path = 'D:\SimBench\statistic'
#merge_csv_files(Output_path)
all_model_list= ["gemma-2-2b-it", "gemma-2-9b-it", "gemma-2-27b-it", "llama-3.1-405b-instruct", "llama-3.1-70b-instruct",
"llama-3.1-8b-instruct", "phi-3-mini-128k-instruct", "phi-3-medium-128k-instruct",
 "nemotron-4-340b-instruct", "mistral-nemo-12b-instruct", "mixtral-8x22b-instruct-v0.1", "codestral-22b-instruct-v0.1",
 "mixtral-8x7b-instruct-v0.1", "mistral-large-latest", "mamba-codestral-7b-v0.1",
 "gpt-4o", "gpt-4o-mini", "claude-3-5-sonnet", "Gemini"]

evaluated_model = "gpt-4o-mini"
test_model_list = ["gpt-4o-mini-f2"]

system_list = ["art", "beam", "buckling", "cable",  "camera", "citybus", "curiosity", "feda", "gator", "gear", "gps_imu", "handler", "hmmwv", "kraz", "lidar", "m113", "man", "mass_spring_damper", "particles", "pendulum",
               "rigid_highway", "rigid_multipatches", "rotor", "scm", "scm_hill", "sedan", "sensros", "slider_crank", "tablecloth", "turtlebot", "uazbus", "veh_app","vehros","viper"]
#system_do_list= ["rotor", "scm", "scm_hill", "sedan", "sensros", "slider_crank", "tablecloth", "turtlebot", "uazbus", "veh_app","vehros","viper"]
system_do_list=system_list
def process_model_system(test_model, system_folder, dataset_path, Output_path, Output_conversation_path,
                         Output_statistic_path):
    system_folder_path = os.path.join(dataset_path, system_folder)
    output_system_path = os.path.join(Output_path, test_model, system_folder)
    os.makedirs(output_system_path, exist_ok=True)

    if system_folder in system_do_list:
        print(f'Processing model {test_model} on system {system_folder}')

        # Read the three response Python files
        first_response_path = os.path.join(output_system_path, "first_response.py")
        second_response_path = os.path.join(output_system_path, "second_response.py")
        third_response_path = os.path.join(output_system_path, "third_response.py")

        first_prediction = read_script(first_response_path)
        second_prediction = read_script(second_response_path)
        third_prediction = read_script(third_response_path)

        first_reference_path = os.path.join(system_folder_path, 'truth1.py')
        second_reference_path = os.path.join(system_folder_path, 'truth2.py')
        third_reference_path = os.path.join(system_folder_path, 'truth3.py')

        first_reference = read_script(first_reference_path)
        second_reference = read_script(second_reference_path)
        third_reference = read_script(third_reference_path)

        api_path = read_script(os.path.join('D:/SimBench/api', 'mbs.txt'))

        # Example usage for first, second, and third rounds
        evaluate_and_save_results("first", first_prediction, first_reference, api_path, output_system_path)
        evaluate_and_save_results("second", second_prediction, second_reference, api_path, output_system_path)
        evaluate_and_save_results("third", third_prediction, third_reference, api_path, output_system_path)

        # Save the scores and metadata to CSV
        save_scores_to_csv_with_metadata(output_system_path, test_model, system_folder)

    return f"Completed {system_folder} for model {test_model}"


# Parallel processing for all models and systems
with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
    futures = []
    for test_model in test_model_list:
        output_model_path = os.path.join(Output_path, test_model)
        os.makedirs(output_model_path, exist_ok=True)

        for system_folder in system_do_list:
            futures.append(
                executor.submit(
                    process_model_system,
                    test_model,
                    system_folder,
                    dataset_path,
                    Output_path,
                    Output_conversation_path,
                    Output_statistic_path
                )
            )

    # Use tqdm to show the progress bar for all futures
    for future in tqdm(as_completed(futures), total=len(futures)):
        print(future.result())

print("Finished processing all models and systems.")

